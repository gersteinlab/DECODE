{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#-----import packages-----#\n",
    "\n",
    "#common python packages\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import wget\n",
    "import math\n",
    "import gc\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "#biological packages\n",
    "import pybedtools\n",
    "from pybedtools import featurefuncs\n",
    "import pyBigWig\n",
    "\n",
    "#machine learning packages\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Flatten, GlobalAveragePooling2D, Multiply\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.utils import Sequence, plot_model\n",
    "from keras.constraints import unit_norm\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, Callback, TensorBoard, ReduceLROnPlateau\n",
    "import keras_metrics as km\n",
    "from keras.models import load_model\n",
    "\n",
    "import cv2\n",
    "\n",
    "#notify the OS about GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SqueezeExcite(tensor, ratio=16):\n",
    "    nb_channel = K.int_shape(tensor)[-1]\n",
    "\n",
    "    x = GlobalAveragePooling2D()(tensor)\n",
    "    x = Dense(nb_channel // ratio, activation='relu')(x)\n",
    "    x = Dense(nb_channel, activation='sigmoid')(x)\n",
    "\n",
    "    x = Multiply()([tensor, x])\n",
    "    return x\n",
    "\n",
    "def build_model():\n",
    "    K.clear_session()\n",
    "    pool2_list = []\n",
    "    merge_list = []\n",
    "\n",
    "    input_size = Input(shape=(5, 200, 1))\n",
    "    conv1_ = Conv2D(128, (3, 10), padding='same',activation='relu')(input_size)\n",
    "    conv1  = SqueezeExcite(conv1_)\n",
    "    conv2_ = Conv2D(64, (3, 1), padding='same',activation='relu')(conv1)\n",
    "    conv2  = SqueezeExcite(conv2_)\n",
    "    conv3_ = Conv2D(64, (3, 3), padding='same',activation='relu')(conv2)\n",
    "    conv3  = SqueezeExcite(conv3_)\n",
    "    conv4_ = Conv2D(128, (3, 1), padding='same',activation='relu')(conv3)\n",
    "    conv4  = SqueezeExcite(conv4_)\n",
    "    pool1  = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    conv5_ = Conv2D(64, (3, 3), padding='same',activation='relu')(pool1)\n",
    "    conv5  = SqueezeExcite(conv5_)\n",
    "    conv6_ = Conv2D(64, (3, 3), padding='same',activation='relu')(conv5)\n",
    "    conv6  = SqueezeExcite(conv6_)\n",
    "    conv7_ = Conv2D(128, (3, 1), padding='same',activation='relu')(conv6)\n",
    "    conv7  = SqueezeExcite(conv7_)\n",
    "    pool2  = MaxPooling2D(pool_size=(2, 2))(conv7)\n",
    "\n",
    "    x = Flatten()(pool2)\n",
    "    dense1_ = Dense(256, activation='relu')\n",
    "    dense1  = dense1_(x)\n",
    "    x = Dropout(0.4)(dense1)\n",
    "    dense2  = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.4)(dense2)\n",
    "    dense3 = Dense(32, activation='relu')(x)\n",
    "    pred_output = Dense(1, activation='sigmoid')(dense3)\n",
    "    model = Model(input=[input_size], output=[pred_output])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all files found!\n",
      "['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
      "all files found!\n"
     ]
    }
   ],
   "source": [
    "#parsing command line arguments\n",
    "# -----parsing command line arguments-----#\n",
    "parser = argparse.ArgumentParser(description='Training CNN model to predict STARR-seq enhancers based on chromatin accessbility and histone marks')\n",
    "parser.add_argument('-w', '--cell_types', type=str, help='comma separated string of cell_types')\n",
    "parser.add_argument('-x', '--in_dir', type=str, help='input_directory')\n",
    "parser.add_argument('-y', '--cell_name', type=str, help='name of the cell')\n",
    "parser.add_argument('-z', '--out_dir', type=str, help='output_directory')\n",
    "parser.add_argument('-a', '--track1_peaks', type=str, help='chromatin accessibility peak')\n",
    "parser.add_argument('-b', '--track2_peaks', type=str, help='ChIP-seq H3K27ac peak')\n",
    "parser.add_argument('-c', '--track3_peaks', type=str, help='ChIP-seq H3K4me3 peak')\n",
    "parser.add_argument('-d', '--track4_peaks', type=str, help='ChIP-seq H3K9ac peak')\n",
    "parser.add_argument('-e', '--track5_peaks', type=str, help='ChIP-seq H3K4me1 peak')\n",
    "parser.add_argument('-f', '--track1_bw', type=str, help='chromatin accessibility bigWig')\n",
    "parser.add_argument('-g', '--track2_bw', type=str, help='ChIP-seq H3K27ac bigWig')\n",
    "parser.add_argument('-i', '--track3_bw', type=str, help='ChIP-seq H3K4me3 bigWig')\n",
    "parser.add_argument('-j', '--track4_bw', type=str, help='ChIP-seq H3K9ac bigWig')\n",
    "parser.add_argument('-k', '--track5_bw', type=str, help='ChIP-seq H3K4me1 bigWig')\n",
    "\n",
    "cell_type = \"NPC\"\n",
    "\n",
    "#simulate command line input\n",
    "seqdir = \"/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/datasets/\" + cell_type + \"/\"\n",
    "cmdline_str='-w ' + \" HepG2,K562,A549,HCT116,MCF-7 \" + \\\n",
    "    ' -x ' + \"/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/\" + \\\n",
    "    ' -y ' + \"NPC\" + \\\n",
    "    ' -z ' + \"/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/\" + \\\n",
    "    ' -a ' + seqdir+cell_type+\".DNase-seq.narrowPeak\" + \\\n",
    "    ' -b ' + seqdir+cell_type+\".ChIP-seq.H3K27ac.narrowPeak\" + \\\n",
    "    ' -c ' + seqdir+cell_type+\".ChIP-seq.H3K4me3.narrowPeak\" + \\\n",
    "    ' -d ' + seqdir+cell_type+\".ChIP-seq.H3K9ac.narrowPeak\" + \\\n",
    "    ' -e ' + seqdir+cell_type+\".ChIP-seq.H3K4me1.narrowPeak\" + \\\n",
    "    ' -f ' + seqdir+cell_type+\".DNase-seq.bigWig\" + \\\n",
    "    ' -g ' + seqdir+cell_type+\".ChIP-seq.H3K27ac.bigWig\" + \\\n",
    "    ' -i ' + seqdir+cell_type+\".ChIP-seq.H3K4me3.bigWig\" + \\\n",
    "    ' -j ' + seqdir+cell_type+\".ChIP-seq.H3K9ac.bigWig\" + \\\n",
    "    ' -k ' + seqdir+cell_type+\".ChIP-seq.H3K4me1.bigWig\"\n",
    "\n",
    "seq_names = [\"DNase\", \"H3K27ac\", \"H3K4me3\", \"H3K9ac\", \"H3K4me1\"]\n",
    "\n",
    "#check if the files are there\n",
    "args = parser.parse_args(cmdline_str.split())\n",
    "args.cell_types = args.cell_types.split(\",\")\n",
    "for cell in args.cell_types:\n",
    "    for seq in seq_names:\n",
    "        pos_file = args.in_dir + cell + \".\" + seq + \".pos.tsv\"\n",
    "        if not os.path.exists(pos_file):\n",
    "            print(pos_file + \" file does not exist\")\n",
    "            exit(1)\n",
    "        neg_file = args.in_dir + cell + \".\" + seq + \".neg.tsv\"\n",
    "        if not os.path.exists(neg_file):\n",
    "            print(neg_file + \" file does not exist\")\n",
    "            exit(1)\n",
    "            \n",
    "for key, value in vars(args).items():\n",
    "    if key == \"cell_types\" or key == \"in_dir\" or key == \"out_dir\" or key == \"cell_name\":\n",
    "        continue\n",
    "    else:\n",
    "        if not os.path.exists(value):\n",
    "            print(key + \" argument file does not exist\")\n",
    "            exit(1)\n",
    "print(\"all files found!\")\n",
    "\n",
    "#construct a set of autosome + X chromosome names\n",
    "chromosomes = []\n",
    "for i in range(1,23):\n",
    "    chromosomes.append(\"chr\"+str(i))\n",
    "chromosomes.append(\"chrX\")\n",
    "print(chromosomes)\n",
    "print(\"all files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auroc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred, curve=\"ROC\", summation_method='careful_interpolation')[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "def auprc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred, curve='PR', summation_method='careful_interpolation')[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(cell_types, in_dir, seq_names):\n",
    "\n",
    "    first_cell = True\n",
    "    for cell in cell_types:\n",
    "        print(cell)\n",
    "\n",
    "        pos = []\n",
    "        neg = []\n",
    "        first_seq = True\n",
    "        for seq in seq_names:\n",
    "            print(\"-\"+seq)\n",
    "\n",
    "            pos_name = in_dir+cell+\".\"+seq+\".pos.tsv\"\n",
    "            pos_mat = np.loadtxt(pos_name, delimiter='\\t')\n",
    "\n",
    "            neg_name = in_dir+cell+\".\"+seq+\".neg.tsv\"\n",
    "            neg_mat = np.loadtxt(neg_name, delimiter='\\t')\n",
    "\n",
    "            if first_seq:\n",
    "                for i in pos_mat:\n",
    "                    pos.append(np.array([i]))\n",
    "                for i in neg_mat:\n",
    "                    neg.append(np.array([i]))\n",
    "                first_seq = False\n",
    "            else:\n",
    "                for i in range(len(pos)):\n",
    "                    pos[i] = np.vstack((pos[i], pos_mat[i,]))\n",
    "                for i in range(len(neg)):\n",
    "                    neg[i] = np.vstack((neg[i], neg_mat[i,]))\n",
    "\n",
    "        if first_cell == True:\n",
    "            X_pos = np.array(pos)\n",
    "            X_neg = np.array(neg)\n",
    "            first_cell = False\n",
    "        else:\n",
    "            X_pos = np.vstack((X_pos, pos))\n",
    "            X_neg = np.vstack((X_neg, neg))\n",
    "\n",
    "    X = np.vstack((X_pos, X_neg))\n",
    "    y = np.array([1 for i in range(X_pos.shape[0])] + [0 for i in range(X_pos.shape[0])]).reshape(-1,1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/datasets/NPC/NPC.DNase-seq.bigWig', '/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/NPC.all.prediction_pos_regions.50.bed']\n",
      "['/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/datasets/NPC/NPC.ChIP-seq.H3K27ac.bigWig', '/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/NPC.all.prediction_pos_regions.50.bed']\n",
      "['/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/datasets/NPC/NPC.ChIP-seq.H3K4me3.bigWig', '/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/NPC.all.prediction_pos_regions.50.bed']\n",
      "['/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/datasets/NPC/NPC.ChIP-seq.H3K9ac.bigWig', '/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/NPC.all.prediction_pos_regions.50.bed']\n",
      "['/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/datasets/NPC/NPC.ChIP-seq.H3K4me1.bigWig', '/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/NPC.all.prediction_pos_regions.50.bed']\n"
     ]
    }
   ],
   "source": [
    "def bigWigAverageOverBed(x, bigwig):\n",
    "    return bigwig.stats(x.chrom, x.start, x.stop, nBins=200)\n",
    "\n",
    "def get_signal(input_list):\n",
    "    print(input_list)\n",
    "    sys.stdout.flush()\n",
    "    return [bigWigAverageOverBed(x, pyBigWig.open(input_list[0])) for x in pybedtools.BedTool(input_list[1])]\n",
    "\n",
    "p = mp.Pool(5)\n",
    "input_list = [[x, args.out_dir + args.cell_name + \".\" + \"all.prediction_pos_regions.50.bed\"] \n",
    "              for x in [args.track1_bw, args.track2_bw, args.track3_bw, args.track4_bw, args.track5_bw]]\n",
    "signal_files = p.map(get_signal, input_list)\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "print(\"finished multiprocess IO\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "#reformat the validation values\n",
    "valid_chromAcc = [np.array(i) for i in signal_files[0]]\n",
    "valid_chip1 = [np.array(i) for i in signal_files[1]]\n",
    "valid_chip2 = [np.array(i) for i in signal_files[2]]\n",
    "valid_chip3 = [np.array(i) for i in signal_files[3]]\n",
    "valid_chip4 = [np.array(i) for i in signal_files[4]]\n",
    "\n",
    "del signal_files\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_pos = pybedtools.BedTool().window_maker(b=args.out_dir + args.cell_name + \".all.prediction_pos_regions.50.bed\", n=200)\n",
    "print(predicted_pos.count())\n",
    "predicted_pos.saveas(args.out_dir + args.cell_name + \".all.prediction_pos_regions.50.breakdown.bed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation = []\n",
    "for i in range(len(valid_chromAcc)):\n",
    "    x_validation.append(np.array([valid_chromAcc[i], valid_chip1[i], valid_chip2[i], valid_chip3[i], valid_chip4[i]]))\n",
    "x_validation = np.nan_to_num(np.array(x_validation, dtype=float))\n",
    "print(x_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    \"\"\"Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    \"\"\"\n",
    "    x = x.copy()\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"Utility function to normalize a tensor by its L2 norm\"\"\"\n",
    "    return (x + 1e-10) / (K.sqrt(K.mean(K.square(x))) + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "x_validation = np.expand_dims(np.array(x_validation), axis=4)\n",
    "# x_validation = np.expand_dims(x_validation, axis=0) #select one image, and expand to proper dimensions\n",
    "print(x_validation.shape)\n",
    "model = load_model('hg38_evaluation.h5', custom_objects={\"auroc\": auroc, \n",
    "                                                         \"auprc\": auroc, \n",
    "                                                         \"f1_m\": f1_m, \n",
    "                                                         \"recall_m\": recall_m,\n",
    "                                                         \"precision_m\": precision_m})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam_pred_multi_1d(model, X):\n",
    "    \"\"\"GradCAM method for visualizing input saliency.\"\"\"\n",
    "    layer_name = \"conv2d_7\"\n",
    "\n",
    "    y_c = model.output\n",
    "    conv_output = model.get_layer(layer_name).output\n",
    "    grads = K.gradients(y_c, conv_output)[0]\n",
    "\n",
    "    # Normalize if necessary\n",
    "    grads = normalize(grads)\n",
    "    gradient_function = K.function([model.input], [conv_output, grads])\n",
    "\n",
    "    output, grads_val = gradient_function([X])\n",
    "    weights = np.mean(grads_val, axis=(1, 2))\n",
    "\n",
    "    all_cam = []\n",
    "    for i in range(len(weights)):\n",
    "        cam = np.dot(output[i], weights[i])\n",
    "        cam = cv2.resize(cam, (200, 1), cv2.INTER_LINEAR)\n",
    "        cam = np.maximum(cam, 0)\n",
    "        all_cam.append(cam[0])\n",
    "\n",
    "    all_cam = np.array(all_cam)\n",
    "    return all_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pred_values = []\n",
    "for i in range(int(len(x_validation)/1000) + 1):\n",
    "    if (i % 10 == 0):\n",
    "        print(i, \",000\")\n",
    "    raw_pred_values.extend(cam_pred_multi_1d(model, x_validation[i * 1000: (i+1) * 1000]))\n",
    "raw_pred_values = np.array(raw_pred_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.describe(raw_pred_values.flatten()))\n",
    "\n",
    "#format into bed with proper regions\n",
    "df = pd.read_csv(args.out_dir + args.cell_name + \".all.prediction_pos_regions.50.breakdown.bed\", sep=\"\\t\",header=None)\n",
    "df[4] = \"bin\"\n",
    "df[5] = raw_pred_values.flatten()\n",
    "df.to_csv(args.out_dir + args.cell_name + \".all.prediction_pos_regions.50.breakdown.raw.bed\", sep=\"\\t\",header=None, index=False)\n",
    "\n",
    "#filter for positive predictions\n",
    "df[df[5]>0.0002].to_csv(args.out_dir + args.cell_name + \".all.prediction_pos_regions.50.breakdown.filtered.bed\", sep=\"\\t\",header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pybedtools.BedTool(args.out_dir + args.cell_name + \".all.prediction_pos_regions.50.breakdown.raw.bed\").sort().merge(c=5, o=\"mean\").saveas(args.out_dir + args.cell_name + \".all.prediction_pos_regions.50.breakdown.raw.bed\")\n",
    "pybedtools.BedTool(args.out_dir + args.cell_name + \".all.prediction_pos_regions.50.breakdown.filtered.bed\").sort().merge(c=5, o=\"mean\").saveas(args.out_dir + args.cell_name + \".all.prediction_pos_regions.50.breakdown.filtered.bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
