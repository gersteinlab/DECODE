{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#-----import packages-----#\n",
    "\n",
    "#common python packages\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import wget\n",
    "import math\n",
    "import gc\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "#biological packages\n",
    "import pybedtools\n",
    "from pybedtools import featurefuncs\n",
    "import pyBigWig\n",
    "\n",
    "#machine learning packages\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Flatten, GlobalAveragePooling2D, Multiply\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.utils import Sequence, plot_model\n",
    "from keras.constraints import unit_norm\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, Callback, TensorBoard, ReduceLROnPlateau\n",
    "import keras_metrics as km\n",
    "from keras.models import load_model\n",
    "\n",
    "#notify the OS about GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SqueezeExcite(tensor, ratio=16):\n",
    "    nb_channel = K.int_shape(tensor)[-1]\n",
    "\n",
    "    x = GlobalAveragePooling2D()(tensor)\n",
    "    x = Dense(nb_channel // ratio, activation='relu')(x)\n",
    "    x = Dense(nb_channel, activation='sigmoid')(x)\n",
    "\n",
    "    x = Multiply()([tensor, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    K.clear_session()\n",
    "    pool2_list = []\n",
    "    merge_list = []\n",
    "\n",
    "    input_size = Input(shape=(5, 200, 1))\n",
    "    conv1_ = Conv2D(128, (3, 10), padding='same',activation='relu')(input_size)\n",
    "    conv1  = SqueezeExcite(conv1_)\n",
    "    conv2_ = Conv2D(64, (3, 1), padding='same',activation='relu')(conv1)\n",
    "    conv2  = SqueezeExcite(conv2_)\n",
    "    conv3_ = Conv2D(64, (3, 3), padding='same',activation='relu')(conv2)\n",
    "    conv3  = SqueezeExcite(conv3_)\n",
    "    conv4_ = Conv2D(128, (3, 1), padding='same',activation='relu')(conv3)\n",
    "    conv4  = SqueezeExcite(conv4_)\n",
    "    pool1  = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    conv5_ = Conv2D(64, (3, 3), padding='same',activation='relu')(pool1)\n",
    "    conv5  = SqueezeExcite(conv5_)\n",
    "    conv6_ = Conv2D(64, (3, 3), padding='same',activation='relu')(conv5)\n",
    "    conv6  = SqueezeExcite(conv6_)\n",
    "    conv7_ = Conv2D(128, (3, 1), padding='same',activation='relu')(conv6)\n",
    "    conv7  = SqueezeExcite(conv7_)\n",
    "    pool2  = MaxPooling2D(pool_size=(2, 2))(conv7)\n",
    "\n",
    "    x = Flatten()(pool2)\n",
    "    dense1_ = Dense(256, activation='relu')\n",
    "    dense1  = dense1_(x)\n",
    "    x = Dropout(0.4)(dense1)\n",
    "    dense2  = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.4)(dense2)\n",
    "    dense3 = Dense(32, activation='relu')(x)\n",
    "    pred_output = Dense(1, activation='sigmoid')(dense3)\n",
    "    model = Model(input=[input_size], output=[pred_output])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred, curve=\"ROC\", summation_method='careful_interpolation')[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "def auprc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred, curve='PR', summation_method='careful_interpolation')[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def bigWigAverageOverBed(x, bigwig):\n",
    "    return bigwig.stats(x.chrom, x.start, x.stop, nBins=200)\n",
    "\n",
    "def get_signal(input_list):\n",
    "    print(input_list)\n",
    "    sys.stdout.flush()\n",
    "    return [bigWigAverageOverBed(x, pyBigWig.open(input_list[0])) for x in pybedtools.BedTool(input_list[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all files found!\n",
      "['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
      "all files found!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #parsing command line arguments\n",
    "    # -----parsing command line arguments-----#\n",
    "    parser = argparse.ArgumentParser(description='Training CNN model to predict STARR-seq enhancers based on chromatin accessbility and histone marks')\n",
    "    parser.add_argument('-w', '--cell_types', type=str, help='comma separated string of cell_types')\n",
    "    parser.add_argument('-x', '--in_dir', type=str, help='input_directory')\n",
    "    parser.add_argument('-y', '--cell_name', type=str, help='name of the cell')\n",
    "    parser.add_argument('-z', '--out_dir', type=str, help='output_directory')\n",
    "    parser.add_argument('-a', '--track1_peaks', type=str, help='chromatin accessibility peak')\n",
    "    parser.add_argument('-b', '--track2_peaks', type=str, help='ChIP-seq H3K27ac peak')\n",
    "    parser.add_argument('-c', '--track3_peaks', type=str, help='ChIP-seq H3K4me3 peak')\n",
    "    parser.add_argument('-d', '--track4_peaks', type=str, help='ChIP-seq H3K9ac peak')\n",
    "    parser.add_argument('-e', '--track5_peaks', type=str, help='ChIP-seq H3K4me1 peak')\n",
    "    parser.add_argument('-f', '--track1_bw', type=str, help='chromatin accessibility bigWig')\n",
    "    parser.add_argument('-g', '--track2_bw', type=str, help='ChIP-seq H3K27ac bigWig')\n",
    "    parser.add_argument('-i', '--track3_bw', type=str, help='ChIP-seq H3K4me3 bigWig')\n",
    "    parser.add_argument('-j', '--track4_bw', type=str, help='ChIP-seq H3K9ac bigWig')\n",
    "    parser.add_argument('-k', '--track5_bw', type=str, help='ChIP-seq H3K4me1 bigWig')\n",
    "\n",
    "    cell_type = \"NPC\"\n",
    "\n",
    "    #simulate command line input\n",
    "    seqdir = \"/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/datasets/\" + cell_type + \"/\"\n",
    "    cmdline_str='-w ' + \" HepG2,K562,A549,HCT116,MCF-7 \" + \\\n",
    "        ' -x ' + \"/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/\" + \\\n",
    "        ' -y ' + \"NPC\" + \\\n",
    "        ' -z ' + \"/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/\" + \\\n",
    "        ' -a ' + seqdir+cell_type+\".DNase-seq.narrowPeak\" + \\\n",
    "        ' -b ' + seqdir+cell_type+\".ChIP-seq.H3K27ac.narrowPeak\" + \\\n",
    "        ' -c ' + seqdir+cell_type+\".ChIP-seq.H3K4me3.narrowPeak\" + \\\n",
    "        ' -d ' + seqdir+cell_type+\".ChIP-seq.H3K9ac.narrowPeak\" + \\\n",
    "        ' -e ' + seqdir+cell_type+\".ChIP-seq.H3K4me1.narrowPeak\" + \\\n",
    "        ' -f ' + seqdir+cell_type+\".DNase-seq.bigWig\" + \\\n",
    "        ' -g ' + seqdir+cell_type+\".ChIP-seq.H3K27ac.bigWig\" + \\\n",
    "        ' -i ' + seqdir+cell_type+\".ChIP-seq.H3K4me3.bigWig\" + \\\n",
    "        ' -j ' + seqdir+cell_type+\".ChIP-seq.H3K9ac.bigWig\" + \\\n",
    "        ' -k ' + seqdir+cell_type+\".ChIP-seq.H3K4me1.bigWig\"\n",
    "\n",
    "    seq_names = [\"DNase\", \"H3K27ac\", \"H3K4me3\", \"H3K9ac\", \"H3K4me1\"]\n",
    "    window_size = 2000\n",
    "\n",
    "    #check if the files are there\n",
    "    args = parser.parse_args(cmdline_str.split())\n",
    "    args.cell_types = args.cell_types.split(\",\")\n",
    "    for cell in args.cell_types:\n",
    "        for seq in seq_names:\n",
    "            pos_file = args.in_dir + cell + \".\" + seq + \".pos.tsv\"\n",
    "            if not os.path.exists(pos_file):\n",
    "                print(pos_file + \" file does not exist\")\n",
    "                exit(1)\n",
    "            neg_file = args.in_dir + cell + \".\" + seq + \".neg.tsv\"\n",
    "            if not os.path.exists(neg_file):\n",
    "                print(neg_file + \" file does not exist\")\n",
    "                exit(1)\n",
    "\n",
    "    for key, value in vars(args).items():\n",
    "        if key == \"cell_types\" or key == \"in_dir\" or key == \"out_dir\" or key == \"cell_name\":\n",
    "            continue\n",
    "        else:\n",
    "            if not os.path.exists(value):\n",
    "                print(key + \" argument file does not exist\")\n",
    "                exit(1)\n",
    "    print(\"all files found!\")\n",
    "\n",
    "    #construct a set of autosome + X chromosome names\n",
    "    chromosomes = []\n",
    "    for i in range(1,23):\n",
    "        chromosomes.append(\"chr\"+str(i))\n",
    "    chromosomes.append(\"chrX\")\n",
    "    print(chromosomes)\n",
    "    print(\"all files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrX\n",
      "78020\n"
     ]
    }
   ],
   "source": [
    "    #generate regions for genome wide predictions\n",
    "    hg38_windows = pybedtools.BedTool().window_maker(genome=\"hg38\", w=window_size)#, s=500)\n",
    "    hg38_windows = hg38_windows.filter(pybedtools.featurefuncs.greater_than, window_size-1)\n",
    "\n",
    "    # temp_chrom = chromosomes[2]\n",
    "    temp_chrom = chromosomes[int(os.environ['chr_num'])]\n",
    "    print(temp_chrom)\n",
    "\n",
    "    hg38_windows = hg38_windows.filter(lambda x: x.chrom == temp_chrom).sort()\n",
    "\n",
    "    #remove ENCODE blacklist regions\n",
    "    if not os.path.exists('./hg38.blacklist.bed.gz'):\n",
    "        url = 'http://mitra.stanford.edu/kundaje/akundaje/release/blacklists/hg38-human/hg38.blacklist.bed.gz'\n",
    "        wget.download(url, './hg38.blacklist.bed.gz')\n",
    "    blacklist = pybedtools.BedTool('./hg38.blacklist.bed.gz')\n",
    "    validation_regions = hg38_windows - blacklist\n",
    "\n",
    "    validation_regions.saveas(args.out_dir + args.cell_name + \".\" + temp_chrom + \".validation_regions.bed\")\n",
    "\n",
    "    print(validation_regions.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/datasets/NPC/NPC.DNase-seq.bigWig', '/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/NPC.chrX.validation_regions.bed']\n",
      "['/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/datasets/NPC/NPC.ChIP-seq.H3K27ac.bigWig', '/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/NPC.chrX.validation_regions.bed']\n",
      "['/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/datasets/NPC/NPC.ChIP-seq.H3K4me3.bigWig', '/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/NPC.chrX.validation_regions.bed']\n",
      "['/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/datasets/NPC/NPC.ChIP-seq.H3K4me1.bigWig', '/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/NPC.chrX.validation_regions.bed']\n",
      "['/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/datasets/NPC/NPC.ChIP-seq.H3K9ac.bigWig', '/gpfs/ysm/scratch60/gerstein/zc264/ChromVar/enhancer-prediction/encode/pipeline/encoded/DNase/NPC.chrX.validation_regions.bed']\n",
      "finished multiprocess IO\n",
      "(78020, 5, 200)\n",
      "(78020, 5, 200, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/ysm/project/zc264/conda_envs/old_keras_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:31: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78020,)\n"
     ]
    }
   ],
   "source": [
    "    #this part is painfully slow due to pyBigWig queries to continuous bigWig file\n",
    "    #perhaps bedtools -> bigWigAverageOverBed is faster?\n",
    "    #for the sake of clarity, the code below is kept\n",
    "\n",
    "    p = mp.Pool(5)\n",
    "    input_list = [[x, args.out_dir + args.cell_name + \".\" + temp_chrom + \".\" + \"validation_regions.bed\"] \n",
    "                  for x in [args.track1_bw, args.track2_bw, args.track3_bw, args.track4_bw, args.track5_bw]]\n",
    "    signal_files = p.map(get_signal, input_list)\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    print(\"finished multiprocess IO\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    #reformat the validation values\n",
    "    valid_chromAcc = [np.array(i) for i in signal_files[0]]\n",
    "    valid_chip1 = [np.array(i) for i in signal_files[1]]\n",
    "    valid_chip2 = [np.array(i) for i in signal_files[2]]\n",
    "    valid_chip3 = [np.array(i) for i in signal_files[3]]\n",
    "    valid_chip4 = [np.array(i) for i in signal_files[4]]\n",
    "    \n",
    "    del signal_files\n",
    "    gc.collect()\n",
    "\n",
    "    x_validation = []\n",
    "    for i in range(validation_regions.count()):\n",
    "        x_validation.append(np.array([valid_chromAcc[i], valid_chip1[i], valid_chip2[i], valid_chip3[i], valid_chip4[i]]))\n",
    "    x_validation = np.nan_to_num(np.array(x_validation, dtype=float))\n",
    "    print(x_validation.shape)\n",
    "\n",
    "    x_validation = np.expand_dims(x_validation, axis=4)\n",
    "    print(x_validation.shape)\n",
    "    model = load_model('hg38_evaluation.h5', custom_objects={\"auroc\": auroc, \n",
    "                                                             \"auprc\": auroc, \n",
    "                                                             \"f1_m\": f1_m, \n",
    "                                                             \"recall_m\": recall_m,\n",
    "                                                             \"precision_m\": precision_m})\n",
    "    y_validation = model.predict(x_validation).ravel()\n",
    "    print(y_validation.shape)\n",
    "\n",
    "    #format into bed with proper regions\n",
    "    df = pd.read_csv(args.out_dir + args.cell_name + \".\" + temp_chrom + \".\" + \"validation_regions.bed\", sep=\"\\t\",header=None)\n",
    "    df[4] = \"pred\"\n",
    "    df[5] = y_validation\n",
    "    df.to_csv(args.out_dir + args.cell_name + \".\" + temp_chrom + \".\" + \"prediction_regions.bed\", sep=\"\\t\",header=None, index=False)\n",
    "    \n",
    "    #filter for positive predictions\n",
    "    df[df[5]>0.5].to_csv(args.out_dir + args.cell_name + \".\" + temp_chrom + \".\" + \"prediction_pos_regions.50.bed\", sep=\"\\t\",header=None, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
